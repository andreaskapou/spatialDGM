{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on spatialVAE code\n",
    "\n",
    "## Likelihood functions\n",
    "In general VAE performance relies strongly on the choice of likelihood function, the 'sum' mode work better compared to taking the 'mean' across the batch. Does the same essentially put more __weight__ on the reconstruction loss and as a consequence downweights the KL term (latent space prior)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy(input=y_hat, target=y, reduction='sum') * size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __spatialVAE__ they use the BCE with logits likelihood function. Not sure why they would want to use this, since the image pixel values are scaled to (0,1). Also they __decoder__ output is not pass through a __Sigmoid__ layer. I changed the code to pass it through Sigmoid instead and use the BCE loss. Looking at few epochs the BCE seemed better but need to test this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-F.binary_cross_entropy_with_logits(input=y_hat, target=y, reduction='sum') * size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Still cannot fully follow the reasoning behind the Decoder implementation. \n",
    "\n",
    "1. The first layer from __latent z__ to hidden layers, they explicitly set bias=False. What is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.coord_linear = nn.Linear(2, hidden_dim) # For xy-coordinates\n",
    "self.latent_linear = nn.Linear(latent_dim, hidden_dim, bias=False) # For z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The decoder part for x-coordinates: they feed in the same coordinates across the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten x so dim is now (batch_size * num_coordinates, 2)\n",
    "x = x.view(batch_size*n, -1)\n",
    "# Pass x coordinates through linear layer to obtain latent space\n",
    "h_x = self.coord_linear(x)\n",
    "\n",
    "# Pass latent z\n",
    "h_z = self.latent_linear(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combine layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each coordinate we add the unstructed and structured elements\n",
    "h = h_x + h_z  # (batch_size, num_coords, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Transform dimensions (flatten) and pass to next layers where the final output size is one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dimensions\n",
    "h = h.view(batch_size * n, -1)\n",
    "\n",
    "y = self.layers(h) # (batch_size*num_coords, output_dim), where output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally they transform to the appropriate size(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num coordinates = data dimensions? that is why we do this here?\n",
    "# Reshape the output appropriately\n",
    "y = y.view(y.size(0), *self.data_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
