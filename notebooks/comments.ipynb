{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on spatialVAE code\n",
    "\n",
    "## Likelihood functions\n",
    "In general VAE performance relies strongly on the choice of likelihood function, the 'sum' mode work better compared to taking the 'mean' across the data points and batch. Does this essentially put more __weight__ on the reconstruction loss and as a consequence downweights the KL term (latent space prior)? This is common when training VAEs though, e.g. see [this blog](http://adamlineberry.ai/vae-series/vae-code-experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_cross_entropy(input=y_hat, target=y, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In __spatialVAE__ they use the BCE with logits likelihood function. Not sure why they would want to use this, since the image pixel values are scaled to (0,1). Also their __decoder__ output is not pass through a __Sigmoid__ layer. I changed the code to pass it through Sigmoid instead and use the BCE loss. Looking at few epochs the BCE seemed better but need to test this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-F.binary_cross_entropy_with_logits(input=y_hat, target=y, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Still cannot fully follow the reasoning behind the Decoder implementation. \n",
    "\n",
    "1. The first layer from __latent z__ to hidden layers, they explicitly set bias=False. What is the reason? Probably because they set it in `coord_linear` and at the end they concatenate them as the input layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.coord_linear = nn.Linear(2, hidden_dim) # For xy-coordinates\n",
    "self.latent_linear = nn.Linear(latent_dim, hidden_dim, bias=False) # For z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The decoder part for x-coordinates: they feed in the same coordinates across the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten x so dim is now (batch_size * num_coordinates, 2)\n",
    "x = x.view(batch_size*n, -1)\n",
    "# Pass x coordinates through linear layer to obtain latent space\n",
    "h_x = self.coord_linear(x)\n",
    "\n",
    "# Pass latent z\n",
    "h_z = self.latent_linear(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Combine layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each coordinate we add the unstructed and structured elements\n",
    "h = h_x + h_z  # (batch_size, num_coords, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Transform dimensions (flatten) and pass to next layers where the final output size is one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dimensions\n",
    "h = h.view(batch_size * n, -1)\n",
    "\n",
    "y = self.layers(h) # (batch_size*num_coords, output_dim), where output_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally they transform to the appropriate size(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num coordinates = data dimensions? that is why we do this here?\n",
    "# Reshape the output appropriately\n",
    "y = y.view(y.size(0), *self.data_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on Pyro implementation\n",
    "\n",
    "## Comparison with implementation in blog post\n",
    "\n",
    "1. In the blog they use a Softplus to constrain the output of the variance parameter to be positive, so then the sampling from the Normal distribution does not have issues with negative variances. Instead, I exponentiate the continuous value obtained from the encoder.\n",
    "\n",
    "2. In the manuscript the authors propose to use a specific prior for the rotation parameter $\\theta$. However, with the Pyro implementation, there is no such specific prior defined. I assume, they consider a Normal prior, similar to all other latent variables. This results at the end of having a different KL divergence and loss function to optimize. __Need to write the equations here to be more specific__.\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
